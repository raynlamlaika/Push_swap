(consepte you should probably know befor read  timr complexity big-O, memory managmment ...)

why we so usr in deffrent algorithms for in deffrent range and why that is the case
is all about the time complexity(i would explaine later)

we woorld esally  see from the visulasetion web sites that is algos like quick sot for the last number are 
waaay faster but in same time it's inefficient for the small data
this is the case cuz of:
    overhead and constant factors
    for the overhead: the sort algo like (quicksort) will need multipale recrusive calls,  which is relatively expensive for small inputs.
    for Example:
            ray: [3, 1, 2]

            1.Quick Sort on [3, 1, 2]:
                Choose pivot (3).
                Partition into [1, 2] (left) and [3] (right).

            2.Quick Sort on [1, 2]:
                Choose pivot (1).
                Partition into [1] and [2].

            3.Recursive calls:
                Quick Sort on [1] (returns immediately).
                Quick Sort on [2] (returns immediately).
                Quick Sort on [3] (returns immediately).

 the multipale recrusive function calls effect the Stack management: 
            Recursion uses additional stack space. In a small dataset, this extra space management (allocation and deallocation) 
            can outweigh the benefits of the algorithm’s core partitioning step, making it slower than simpler algorithms.

on more thing that we need to take about it is the constant factors:
constant factorsr :are referring to the hidden costs that aren't reflected directly 
                    in the big-O time complexity but still influence how the algorithm performs in practice.

Constant Factors are the overheads and EXTRA operations that aren’t captured in the big-O notation. These include things like:
The time it takes to perform recursive calls (for Quick Sort).
The time it takes to swap elements.
The time spent on partitioning the array.
Any overhead due to memory management or function calls.

here we can see the cost of the effencancy of the recrusive func :
However, this power comes at a cost, specifically in terms of memory usage.
Each time a recursive function is called, a new layer is added to the system's stack. 
This layer contains information about the current state of the function, including any variables and their values. 
As the recursion deepens, more and more of these layers are added, each consuming a portion of the system's memory.














left shift and right shit in code  << >> they are used to move the bits 
of some var to the left || right 

Left shift:
int a = 5;  // 5 in binary: 0000 0101
int result = a << 1;  // Left shift by 1: 0000 1010 (which is 10 in decimal)

Right shift:
int a = 5;  // 5 in binary: 0000 0101
int result = a >> 1;  // Right shift by 1: 0000 0010 (which is 2 in decimal)
